{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.23.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\requests-2.28.1-py3.10.egg (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\certifi-2022.6.15-py3.10.egg (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\charset_normalizer-2.1.0-py3.10.egg (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\idna-3.3-py3.10.egg (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\urllib3-1.26.10-py3.10.egg (from requests->transformers) (1.26.10)\n",
      "Collecting contractionsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "     ------------------------------------ 287.5/287.5 KB 572.8 kB/s eta 0:00:00\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\65831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\65831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\65831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    text = text.replace(\"'s\",\"\")\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def remove_special_character(text):\n",
    "    text = text.replace('\\n', ' ') \n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = ' '.join([lem.lemmatize(x, pos = 'v') for x in text.split()])\n",
    "    return corpus\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_special_character(text)\n",
    "    text = lemmatize(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_chunk_process(df) : \n",
    "    df = df.copy()\n",
    "    print(\"Processing chunk\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Processed Title'] = df['Title'].apply(lambda x: preprocess_text(x))\n",
    "    # df['Processed Text'] = df['Text'].apply(lambda x: preprocess_text(x))\n",
    "   \n",
    "    # title\n",
    "    news_title = df['Processed Title'].to_list()\n",
    "    inputs = tokenizer(news_title, padding = True, truncation = True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    predictions_title = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    model.config.id2label\n",
    "\n",
    "    df['Positive_Title'] = predictions_title[:, 0].tolist()\n",
    "    df['Negative_Title'] = predictions_title[:, 1].tolist()\n",
    "    df['Neutral_Title'] = predictions_title[:, 2].tolist()\n",
    "\n",
    "    # # text\n",
    "    # news_text = df['Processed Text'].to_list()\n",
    "    # inputs = tokenizer(news_text, padding = True, truncation = True, return_tensors='pt')\n",
    "    # outputs = model(**inputs)\n",
    "\n",
    "    # predictions_text = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # model.config.id2label\n",
    "\n",
    "    # df['Positive_Text'] = predictions_text[:, 0].tolist()\n",
    "    # df['Negative_Text'] = predictions_text[:, 1].tolist()\n",
    "    # df['Neutral_Text'] = predictions_text[:, 2].tolist()\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def finbert_process(df, chunk_size=50):\n",
    "    print(\"Breaking data into smaller chunks\")\n",
    "    chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)] \n",
    "\n",
    "    lst = []\n",
    "    for chunk in chunks:\n",
    "        lst.append(finbert_chunk_process(chunk))\n",
    "    \n",
    "    output = pd.concat(lst)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_aggregator(df, title = True, type = \"abs_max\"):\n",
    "    \"\"\"\n",
    "    Aggregates sentiments on a per day basis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Dataset generated after sentiment analysis.\n",
    "    title: boolean\n",
    "        To indicate if the news title or news body text is used to generate the aggregated sentiment. \n",
    "        Default is True (ie. News title is used for aggregated sentiment)\n",
    "    type: Str {\"mean\", \"abs_max\"}\n",
    "        To indicate method of calculation.\n",
    "        \"mean\": Group by Date and takes mean of \"Compound\"\n",
    "        \"abs_max\": Calculates the absolute max of \"Positive\" and \"Negative\" column. Then group by Date and takes mean of this new column\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Output : Series\n",
    "        Contains aggregated sentiment for each day\n",
    "    \"\"\"\n",
    "    if title:\n",
    "        positive = \"Positive_Title\"\n",
    "        negative = \"Negative_Title\"\n",
    "    else:\n",
    "        positive = \"Positive_Text\"\n",
    "        negative = \"Negative_Text\"\n",
    "\n",
    "    if type == \"mean\":\n",
    "        df[\"Compound\"] = df[positive] - df[negative]\n",
    "        return df.groupby('Date')['Compound'].aggregate('mean')\n",
    "\n",
    "    elif type == \"abs_max\":\n",
    "        df['Negative'] = -df[negative]\n",
    "        df['Positive'] = df[positive]\n",
    "        df['Sentiment'] = df.apply(lambda x: max(x[\"Negative\"], x[\"Positive\"], key=abs), axis=1)\n",
    "\n",
    "        return df.groupby('Date')['Sentiment'].aggregate('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_data():\n",
    "    df_world= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/world_news.csv')\n",
    "    df_politics= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/politics_news.csv')\n",
    "    df_coronavirus= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/coronavirus_news.csv')\n",
    "    df_aapl= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/aapl_news.csv')\n",
    "    df_meta= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/meta_news.csv')\n",
    "    df_tsla= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/tsla_news.csv')\n",
    "\n",
    "    # Removing duplicates\n",
    "    df_world = df_world.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    df_politics = df_politics.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    df_coronavirus = df_coronavirus.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    df_aapl = df_aapl.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    df_meta = df_meta.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    df_tsla = df_tsla.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    \n",
    "    # Drop any Nan \n",
    "    df_world = df_world.dropna()\n",
    "    df_politics = df_politics.dropna()\n",
    "    df_coronavirus = df_coronavirus.dropna()\n",
    "    df_aapl = df_aapl.dropna()\n",
    "    df_meta = df_meta.dropna()\n",
    "    df_tsla = df_tsla.dropna()\n",
    "    \n",
    "    return df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla\n",
    "    \n",
    "def finbert_all():\n",
    "    df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla = clean_all_data()\n",
    "    \n",
    "    df_aapl = finbert_process(df_aapl)\n",
    "    df_meta = finbert_process(df_meta)\n",
    "    df_tsla = finbert_process(df_tsla)\n",
    "    df_world = finbert_process(df_world)\n",
    "    df_politics = finbert_process(df_politics)\n",
    "    df_coronavirus = finbert_process(df_coronavirus)\n",
    "\n",
    "    df_aapl.to_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_finbert.csv',index=False)\n",
    "    df_meta.to_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_finbert.csv',index=False)\n",
    "    df_tsla.to_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_finbert.csv',index=False)\n",
    "    df_world.to_csv('../Data/2_Processed/Unstructured_Data/Global/world_finbert.csv',index=False)\n",
    "    df_politics.to_csv('../Data/2_Processed/Unstructured_Data/Global/politics_finbert.csv',index=False)\n",
    "    df_coronavirus.to_csv('../Data/2_Processed/Unstructured_Data/Global/coronavirus_finbert.csv',index=False)\n",
    "\n",
    "def aggregate_sentiment_all(title, type='abs_max'):\n",
    "    df_world_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/world_finbert.csv')\n",
    "    df_politics_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/politics_finbert.csv')\n",
    "    df_coronavirus_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/coronavirus_finbert.csv')\n",
    "    df_aapl_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_finbert.csv')\n",
    "    df_meta_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_finbert.csv')\n",
    "    df_tsla_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_finbert.csv')\n",
    "\n",
    "    aggregated_sentiment_aapl = sentiment_aggregator(df_aapl_finbert, title=title, type=type)\n",
    "    aggregated_sentiment_meta = sentiment_aggregator(df_meta_finbert, title=title, type=type)\n",
    "    aggregated_sentiment_tsla = sentiment_aggregator(df_tsla_finbert, title=title, type=type)\n",
    "    aggregated_sentiment_world = sentiment_aggregator(df_world_finbert, title=title, type=type)\n",
    "    aggregated_sentiment_politics = sentiment_aggregator(df_politics_finbert, title=title, type=type)\n",
    "    aggregated_sentiment_coronavirus = sentiment_aggregator(df_coronavirus_finbert, title=title, type=type)\n",
    "\n",
    "    lst = [aggregated_sentiment_aapl, aggregated_sentiment_meta, aggregated_sentiment_tsla, aggregated_sentiment_world, aggregated_sentiment_politics, aggregated_sentiment_coronavirus]\n",
    "    keys = [\"AAPL\", \"META\", \"TSLA\", \"World\", \"Politics\", \"Coronavirus\"]\n",
    "    \n",
    "    return pd.concat(lst, keys=keys, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\FinBERT-News-Analysis.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m finbert_all()\n",
      "\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\FinBERT-News-Analysis.ipynb Cell 9\u001b[0m in \u001b[0;36mfinbert_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinbert_all\u001b[39m():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla \u001b[39m=\u001b[39m clean_all_data()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# df_aapl = finbert_process(df_aapl)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# df_meta = finbert_process(df_meta)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# df_tsla = finbert_process(df_tsla)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     df_world \u001b[39m=\u001b[39m finbert_process(df_world)\n",
      "\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\FinBERT-News-Analysis.ipynb Cell 9\u001b[0m in \u001b[0;36mclean_all_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_all_data\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df_world\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../Data/1_Scraped/Unstructured_Data/Global/world_news.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df_politics\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../Data/1_Scraped/Unstructured_Data/Global/politics_news.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/FinBERT-News-Analysis.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df_coronavirus\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../Data/1_Scraped/Unstructured_Data/Global/coronavirus_news.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "finbert_all() #Takes quite long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>META</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>World</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Coronavirus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>-0.102317</td>\n",
       "      <td>-0.328605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.187394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.279706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.237588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.189436</td>\n",
       "      <td>-0.092877</td>\n",
       "      <td>0.084386</td>\n",
       "      <td>-0.297506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>0.160202</td>\n",
       "      <td>-0.088187</td>\n",
       "      <td>0.220575</td>\n",
       "      <td>-0.206285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.042963</td>\n",
       "      <td>-0.392491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.288284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.132247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.315690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>-0.208962</td>\n",
       "      <td>0.340929</td>\n",
       "      <td>-0.290627</td>\n",
       "      <td>-0.102536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.429384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-08</th>\n",
       "      <td>0.063883</td>\n",
       "      <td>-0.400615</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>-0.200342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.053323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09</th>\n",
       "      <td>0.606935</td>\n",
       "      <td>-0.154664</td>\n",
       "      <td>0.032192</td>\n",
       "      <td>-0.137133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1741 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      META      TSLA     World  Politics  Coronavirus\n",
       "Date                                                                     \n",
       "2018-01-03 -0.102317 -0.328605       NaN -0.187394       NaN          NaN\n",
       "2018-01-04       NaN -0.279706       NaN -0.237588       NaN          NaN\n",
       "2018-01-05  0.189436 -0.092877  0.084386 -0.297506       NaN          NaN\n",
       "2018-01-06  0.160202 -0.088187  0.220575 -0.206285       NaN          NaN\n",
       "2018-01-07       NaN       NaN -0.042963 -0.392491       NaN          NaN\n",
       "...              ...       ...       ...       ...       ...          ...\n",
       "2022-12-06       NaN -0.288284       NaN -0.132247       NaN    -0.315690\n",
       "2022-12-07 -0.208962  0.340929 -0.290627 -0.102536       NaN     0.429384\n",
       "2022-12-08  0.063883 -0.400615  0.024500 -0.200342       NaN    -0.053323\n",
       "2022-12-09  0.606935 -0.154664  0.032192 -0.137133       NaN          NaN\n",
       "2022-12-10       NaN  0.024467       NaN       NaN       NaN          NaN\n",
       "\n",
       "[1741 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = aggregate_sentiment_all(title=True)\n",
    "df_all = df_all.sort_values(by=\"Date\")\n",
    "df_all.to_csv('../Data/2_Processed/Unstructured_Data/All/all_finbert.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "992e3c1ac2fab5c819af78d2f68745e04c538b3e95445f87071a441f9b70fbfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
