{"cells":[{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2484,"status":"ok","timestamp":1664779678534,"user":{"displayName":"Ashley Ho","userId":"17045796355417485048"},"user_tz":-480},"id":"3RcZ27W4T-c9","outputId":"1d1bb1f5-a5cd-4978-938a-c2cc6c0f4804"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package names to /Users/eltontay/nltk_data...\n","[nltk_data]   Package names is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/eltontay/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /Users/eltontay/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     /Users/eltontay/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","[nltk_data] Downloading package punkt to /Users/eltontay/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","\n","nltk.download([\n","    \"names\",\n","    \"stopwords\",\n","    \"averaged_perceptron_tagger\",\n","    \"vader_lexicon\",\n","    \"punkt\",\n","    ])"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1712,"status":"ok","timestamp":1664779680238,"user":{"displayName":"Ashley Ho","userId":"17045796355417485048"},"user_tz":-480},"id":"cvyNoXiquVGx","outputId":"f1b1ad36-446b-4e30-a7a9-b95a16857c9d"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     /Users/eltontay/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/eltontay/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/eltontay/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","import json\n","\n","import nltk\n","nltk.download('omw-1.4')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","import string\n","import re"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0g8OyI4Y35Ny"},"outputs":[],"source":["def remove_contractions(text):\n","    text = text.replace(\"'s\",\"\")\n","    return text\n","\n","def remove_punctuation(text):\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text\n","\n","def lowercase(text):\n","    return text.lower()\n","\n","def remove_stopwords(text):\n","    stopwords = nltk.corpus.stopwords.words('english')\n","    text = ' '.join([word for word in text.split() if word not in stopwords])\n","    return text\n","\n","def remove_special_character(text):\n","    text = text.replace('\\n', ' ') \n","    return text\n","\n","def lemmatize(text):\n","    lem = WordNetLemmatizer()\n","    corpus = ' '.join([lem.lemmatize(x, pos = 'v') for x in text.split()])\n","    return corpus\n","\n","def preprocess_text(text):\n","    text = remove_contractions(text)\n","    text = remove_punctuation(text)\n","    text = lowercase(text)\n","    text = remove_stopwords(text)\n","    text = remove_special_character(text)\n","    text = lemmatize(text)\n","    return text\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'SentimentIntensityAnalyzer' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\Vader-News-Analysis.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Vader-News-Analysis.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sia \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Vader-News-Analysis.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvader_process\u001b[39m(df) : \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Vader-News-Analysis.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m])\n","\u001b[1;31mNameError\u001b[0m: name 'SentimentIntensityAnalyzer' is not defined"]}],"source":["sia = SentimentIntensityAnalyzer()\n","\n","def vader_process(df) : \n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df['Processed Title'] = df['Title'].apply(lambda x: preprocess_text(x))\n","    # df['Processed Text'] = df['Text'].apply(lambda x: preprocess_text(x))\n","    df['Sentiment Title'] = df['Processed Title'].apply(lambda x: sia.polarity_scores(x))\n","    # df['Sentiment Text'] = df['Processed Text'].apply(lambda x: sia.polarity_scores(x))\n","    # df['Positive Title'] = df['Sentiment Title'].apply(lambda x: x[\"compound\"] > 0)\n","    # df['Positive Text'] = df['Sentiment Text'].apply(lambda x: x[\"compound\"] > 0)\n","    return df"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["def sentiment_aggregator(df, title = True, type=\"mean\"):\n","    \"\"\"\n","    Aggregates sentiments on a per day basis.\n","\n","    Parameters\n","    ----------\n","    df: DataFrame\n","        Dataset generated after sentiment analysis.\n","    title: boolean\n","        To indicate if the news title or news body text is used to generate the aggregated sentiment. \n","        Default is True (ie. News title is used for aggregated sentiment)\n","    type: Str {\"mean\", \"abs_max\"}\n","        To indicate method of calculation.\n","        \"mean\": Group by Date and takes mean of \"Compound\"\n","        \"abs_max\": Calculates the absolute max of \"Positive\" and \"Negative\" column. Then group by Date and takes mean of this new column\n","\n","    Returns\n","    -------\n","    Output : Series\n","        Contains aggregated sentiment for each day\n","    \"\"\"\n","   \n","    target = \"Sentiment Title\"\n","    #  if title else \"Sentiment Text\"\n","    \n","    df[target] = df[target].str.replace('\\'','\\\"')\n","    df[target] = df[target].apply(lambda x: json.loads(x))\n","\n","    df['Negative'] = df[target].apply(lambda x: x.get('neg'))\n","    df['Neutral'] = df[target].apply(lambda x: x.get('neu'))\n","    df['Positive'] = df[target].apply(lambda x: x.get('pos'))\n","    df['Compound'] = df[target].apply(lambda x: x.get('compound'))\n","\n","    if type == \"mean\":\n","        return df.groupby('Date')['Compound'].aggregate('mean')\n","\n","    elif type == \"abs_max\":\n","        df['Negative'] = df[target].apply(lambda x: -x.get('neg'))\n","        df['Sentiment'] = df.apply(lambda x: max(x['Negative'], x['Positive'], key=abs), axis=1)\n","\n","        return df.groupby('Date')['Sentiment'].aggregate('mean')\n"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["def clean_all_data():\n","    df_world= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/world_news.csv')\n","    df_politics= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/politics_news.csv')\n","    df_coronavirus= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/coronavirus_news.csv')\n","    df_aapl= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/aapl_news.csv')\n","    df_meta= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/meta_news.csv')\n","    df_tsla= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/tsla_news.csv')\n","\n","    # Removing duplicates\n","    df_world = df_world.drop_duplicates(subset=['Date', 'Title'], keep='first')\n","    df_politics = df_politics.drop_duplicates(subset=['Date', 'Title'], keep='first')\n","    df_coronavirus = df_coronavirus.drop_duplicates(subset=['Date', 'Title'], keep='first')\n","    df_aapl = df_aapl.drop_duplicates(subset=['Date', 'Title'], keep='first')\n","    df_meta = df_meta.drop_duplicates(subset=['Date', 'Title'], keep='first')\n","    df_tsla = df_tsla.drop_duplicates(subset=['Date', 'Title'], keep='first')\n","    \n","    # Further filtering based on keyword\n","    # aapl_keyword = ['Apple', 'iPhone', 'Macbook', 'iPad']\n","    # meta_keyword = ['Facebook', 'Meta', 'Metaverse']\n","    # tsla_keyword = ['Tesla', 'EV', 'Elon', 'Musk']\n","\n","    # df_aapl = df_aapl[df_aapl['Title'].str.contains('|'.join(aapl_keyword)) == True]\n","    # df_meta = df_meta[df_meta['Title'].str.contains('|'.join(meta_keyword)) == True]\n","    # df_tsla = df_tsla[df_tsla['Title'].str.contains('|'.join(tsla_keyword)) == True]\n","\n","    # Drop any Nan \n","    df_world = df_world.dropna()\n","    df_politics = df_politics.dropna()\n","    df_coronavirus = df_coronavirus.dropna()\n","    df_aapl = df_aapl.dropna()\n","    df_meta = df_meta.dropna()\n","    df_tsla = df_tsla.dropna()\n","    \n","    return df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla\n","\n","def vader_all():\n","    df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla = clean_all_data()\n","    \n","    df_aapl = vader_process(df_aapl)\n","    df_meta = vader_process(df_meta)\n","    df_tsla = vader_process(df_tsla)\n","    df_world = vader_process(df_world)\n","    df_politics = vader_process(df_politics)\n","    df_coronavirus = vader_process(df_coronavirus)\n","\n","    df_aapl.to_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_vader.csv',index=False)\n","    df_meta.to_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_vader.csv',index=False)\n","    df_tsla.to_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_vader.csv',index=False)\n","    df_world.to_csv('../Data/2_Processed/Unstructured_Data/Global/world_vader.csv',index=False)\n","    df_politics.to_csv('../Data/2_Processed/Unstructured_Data/Global/politics_vader.csv',index=False)\n","    df_coronavirus.to_csv('../Data/2_Processed/Unstructured_Data/Global/coronavirus_vader.csv',index=False)\n","    \n","\n","def aggregate_sentiment_all(title, type):\n","    df_world_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/world_vader.csv')\n","    df_politics_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/politics_vader.csv')\n","    df_coronavirus_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/coronavirus_vader.csv')\n","    df_aapl_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_vader.csv')\n","    df_meta_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_vader.csv')\n","    df_tsla_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_vader.csv')\n","\n","    aggregated_sentiment_aapl = sentiment_aggregator(df_aapl_vader, title=title, type=type)\n","    aggregated_sentiment_meta = sentiment_aggregator(df_meta_vader, title=title, type=type)\n","    aggregated_sentiment_tsla = sentiment_aggregator(df_tsla_vader, title=title, type=type)\n","    aggregated_sentiment_world = sentiment_aggregator(df_world_vader, title=title, type=type)\n","    aggregated_sentiment_politics = sentiment_aggregator(df_politics_vader, title=title, type=type)\n","    aggregated_sentiment_coronavirus = sentiment_aggregator(df_coronavirus_vader, title=title, type=type)\n","\n","    lst = [aggregated_sentiment_aapl, aggregated_sentiment_meta, aggregated_sentiment_tsla, aggregated_sentiment_world, aggregated_sentiment_politics, aggregated_sentiment_coronavirus]\n","    keys = [\"AAPL\", \"META\", \"TSLA\", \"World\", \"Politics\", \"Coronavirus\"]\n","    \n","    return pd.concat(lst, keys=keys, axis=1)\n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["vader_all()"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["#vader_all()\n","df_all = aggregate_sentiment_all(title=True, type=\"abs_max\")\n","df_all = df_all.sort_values(by=\"Date\")\n","df_all.to_csv('../Data/2_Processed/Unstructured_Data/All/all_vader.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"992e3c1ac2fab5c819af78d2f68745e04c538b3e95445f87071a441f9b70fbfa"}}},"nbformat":4,"nbformat_minor":0}
