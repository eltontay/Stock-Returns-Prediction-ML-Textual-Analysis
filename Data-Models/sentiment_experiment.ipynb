{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/world_finbert.csv')\n",
    "df_politics_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/politics_finbert.csv')\n",
    "df_coronavirus_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/coronavirus_finbert.csv')\n",
    "df_aapl_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_finbert.csv')\n",
    "df_meta_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_finbert.csv')\n",
    "df_tsla_finbert= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_finbert.csv')\n",
    "\n",
    "df_world_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/world_flair.csv')\n",
    "df_politics_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/politics_flair.csv')\n",
    "df_coronavirus_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global//coronavirus_flair.csv')\n",
    "df_aapl_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_flair.csv')\n",
    "df_meta_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_flair.csv')\n",
    "df_tsla_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_flair.csv')\n",
    "\n",
    "df_world_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/world_vader.csv')\n",
    "df_politics_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/politics_vader.csv')\n",
    "df_coronavirus_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/coronavirus_vader.csv')\n",
    "df_aapl_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_vader.csv')\n",
    "df_meta_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_vader.csv')\n",
    "df_tsla_vader= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_vader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "def downloadHistory(tickerName, start=\"2018-01-01\", end=\"2022-09-30\"):\n",
    "    return yf.download(tickerName, start=start, end=end)[['Open', 'High', 'Low', 'Close', 'Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "aapl_hist = downloadHistory('AAPL')\n",
    "meta_hist = downloadHistory('META')\n",
    "tsla_hist = downloadHistory('TSLA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_keyword = ['aapl', 'apple', 'iphone', 'macbook', 'ipad', 'ios']\n",
    "meta_keyword = ['fb', 'facebook', 'meta', 'metaverse']\n",
    "tsla_keyword = ['tsla', 'tesla', 'ev', 'elon', 'musk', 'electric car']\n",
    "\n",
    "df_aapl_finbert_filtered = df_aapl_finbert[df_aapl_finbert['Processed Title'].str.contains('|'.join(aapl_keyword)) == True]\n",
    "df_meta_finbert_filtered = df_meta_finbert[df_meta_finbert['Processed Title'].str.contains('|'.join(meta_keyword)) == True]\n",
    "df_tsla_finbert_filtered = df_tsla_finbert[df_tsla_finbert['Processed Title'].str.contains('|'.join(tsla_keyword)) == True]\n",
    "\n",
    "df_aapl_flair_filtered = df_aapl_flair[df_aapl_flair['Processed Title'].str.contains('|'.join(aapl_keyword)) == True]\n",
    "df_meta_flair_filtered = df_meta_flair[df_meta_flair['Processed Title'].str.contains('|'.join(meta_keyword)) == True]\n",
    "df_tsla_flair_filtered = df_tsla_flair[df_tsla_flair['Processed Title'].str.contains('|'.join(tsla_keyword)) == True]\n",
    "\n",
    "df_aapl_vader_filtered = df_aapl_vader[df_aapl_vader['Processed Title'].str.contains('|'.join(aapl_keyword)) == True]\n",
    "df_meta_vader_filtered = df_meta_vader[df_meta_vader['Processed Title'].str.contains('|'.join(meta_keyword)) == True]\n",
    "df_tsla_vader_filtered = df_tsla_vader[df_tsla_vader['Processed Title'].str.contains('|'.join(tsla_keyword)) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\65831\\AppData\\Local\\Temp\\ipykernel_22268\\3221107698.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Negative'] = -df[negative]\n",
      "C:\\Users\\65831\\AppData\\Local\\Temp\\ipykernel_22268\\3221107698.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Positive'] = df[positive]\n",
      "C:\\Users\\65831\\AppData\\Local\\Temp\\ipykernel_22268\\3221107698.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Sentiment'] = df.apply(lambda x: max(x[\"Negative\"], x[\"Positive\"], key=abs), axis=1)\n",
      "C:\\Users\\65831\\AppData\\Local\\Temp\\ipykernel_22268\\3221107698.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Compound\"] = df[positive] - df[negative]\n"
     ]
    }
   ],
   "source": [
    "def sentiment_aggregator(df, title = True, type = \"abs_max\"):\n",
    "    \"\"\"\n",
    "    Aggregates sentiments on a per day basis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Dataset generated after sentiment analysis.\n",
    "    title: boolean\n",
    "        To indicate if the news title or news body text is used to generate the aggregated sentiment. \n",
    "        Default is True (ie. News title is used for aggregated sentiment)\n",
    "    type: Str {\"mean\", \"abs_max\"}\n",
    "        To indicate method of calculation.\n",
    "        \"mean\": Group by Date and takes mean of \"Compound\"\n",
    "        \"abs_max\": Calculates the absolute max of \"Positive\" and \"Negative\" column. Then group by Date and takes mean of this new column\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Output : Series\n",
    "        Contains aggregated sentiment for each day\n",
    "    \"\"\"\n",
    "    if title:\n",
    "        positive = \"Positive_Title\"\n",
    "        negative = \"Negative_Title\"\n",
    "    else:\n",
    "        positive = \"Positive_Text\"\n",
    "        negative = \"Negative_Text\"\n",
    "\n",
    "    if type == \"mean\":\n",
    "        df[\"Compound\"] = df[positive] - df[negative]\n",
    "        return df.groupby('Date')['Compound'].aggregate('mean')\n",
    "\n",
    "    elif type == \"abs_max\":\n",
    "        df['Negative'] = -df[negative]\n",
    "        df['Positive'] = df[positive]\n",
    "        df['Sentiment'] = df.apply(lambda x: max(x[\"Negative\"], x[\"Positive\"], key=abs), axis=1)\n",
    "\n",
    "        return df.groupby('Date')['Sentiment'].aggregate('mean')\n",
    "\n",
    "\n",
    "# Finbert\n",
    "meta_finbert_maxabs_df = sentiment_aggregator(df_meta_finbert_filtered, title = True, type = \"abs_max\") \n",
    "aapl_finbert_maxabs_df = sentiment_aggregator(df_aapl_finbert_filtered, title = True, type = \"abs_max\") \n",
    "tsla_finbert_maxabs_df = sentiment_aggregator(df_tsla_finbert_filtered, title = True, type = \"abs_max\") \n",
    "\n",
    "meta_finbert_mean_df = sentiment_aggregator(df_meta_finbert_filtered, title = True, type = \"mean\") \n",
    "aapl_finbert_mean_df = sentiment_aggregator(df_aapl_finbert_filtered, title = True, type = \"mean\") \n",
    "tsla_finbert_mean_df = sentiment_aggregator(df_tsla_finbert_filtered, title = True, type = \"mean\") \n",
    "\n",
    "df_world_finbert_maxabs = sentiment_aggregator(df_world_finbert, title = True, type = \"abs_max\") \n",
    "df_politics_finbert_maxabs = sentiment_aggregator(df_politics_finbert, title = True, type = \"abs_max\") \n",
    "df_coronavirus_finbert_maxabs = sentiment_aggregator(df_coronavirus_finbert, title = True, type = \"abs_max\") \n",
    "\n",
    "df_world_finbert_mean = sentiment_aggregator(df_world_finbert, title = True, type = \"mean\") \n",
    "df_politics_finbert_mean = sentiment_aggregator(df_politics_finbert, title = True, type = \"mean\") \n",
    "df_coronavirus_finbert_mean = sentiment_aggregator(df_coronavirus_finbert, title = True, type = \"mean\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_aggregator(df, title = True):\n",
    "    # flair only gives one value and a label (POSITIVE or NEGATIVE) so we just use mean\n",
    "    # if title:\n",
    "    return df.groupby('Date')['Score'].aggregate('mean')\n",
    "\n",
    "    # else:\n",
    "    #     return df.groupby('Date')['Score Text'].aggregate('mean')\n",
    "        \n",
    "        \n",
    "# Flair\n",
    "meta_flair_df = sentiment_aggregator(df_meta_flair_filtered, title = True) \n",
    "aapl_flair_df = sentiment_aggregator(df_aapl_flair_filtered, title = True) \n",
    "tsla_flair_df = sentiment_aggregator(df_tsla_flair_filtered, title = True) \n",
    "\n",
    "df_world_flair = sentiment_aggregator(df_world_flair, title = True) \n",
    "df_politics_flair = sentiment_aggregator(df_politics_flair, title = True) \n",
    "df_coronavirus_flair = sentiment_aggregator(df_coronavirus_flair, title = True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader\n",
    "def sentiment_aggregator(df, title = True, type=\"mean\"):\n",
    "    \"\"\"\n",
    "    Aggregates sentiments on a per day basis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Dataset generated after sentiment analysis.\n",
    "    title: boolean\n",
    "        To indicate if the news title or news body text is used to generate the aggregated sentiment. \n",
    "        Default is True (ie. News title is used for aggregated sentiment)\n",
    "    type: Str {\"mean\", \"abs_max\"}\n",
    "        To indicate method of calculation.\n",
    "        \"mean\": Group by Date and takes mean of \"Compound\"\n",
    "        \"abs_max\": Calculates the absolute max of \"Positive\" and \"Negative\" column. Then group by Date and takes mean of this new column\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Output : Series\n",
    "        Contains aggregated sentiment for each day\n",
    "    \"\"\"\n",
    "   \n",
    "    target = \"Sentiment Title\"\n",
    "    #  if title else \"Sentiment Text\"\n",
    "    df = df.copy()\n",
    "    df[target] = df[target].str.replace('\\'','\\\"')\n",
    "    df[target] = df[target].apply(lambda x: json.loads(x))\n",
    "\n",
    "    df['Negative'] = df[target].apply(lambda x: x.get('neg'))\n",
    "    df['Neutral'] = df[target].apply(lambda x: x.get('neu'))\n",
    "    df['Positive'] = df[target].apply(lambda x: x.get('pos'))\n",
    "    df['Compound'] = df[target].apply(lambda x: x.get('compound'))\n",
    "\n",
    "    if type == \"mean\":\n",
    "        return df.groupby('Date')['Compound'].aggregate('mean')\n",
    "\n",
    "    elif type == \"abs_max\":\n",
    "        df['Negative'] = df[target].apply(lambda x: -x.get('neg'))\n",
    "        df['Sentiment'] = df.apply(lambda x: max(x['Negative'], x['Positive'], key=abs), axis=1)\n",
    "\n",
    "        return df.groupby('Date')['Sentiment'].aggregate('mean')\n",
    "\n",
    "\n",
    "# Vader\n",
    "meta_vader_maxabs_df = sentiment_aggregator(df_meta_vader_filtered, title = True, type = \"abs_max\") \n",
    "aapl_vader_maxabs_df = sentiment_aggregator(df_aapl_vader_filtered, title = True, type = \"abs_max\") \n",
    "tsla_vader_maxabs_df = sentiment_aggregator(df_tsla_vader_filtered, title = True, type = \"abs_max\") \n",
    "\n",
    "meta_vader_mean_df = sentiment_aggregator(df_meta_vader_filtered, title = True, type = \"mean\") \n",
    "aapl_vader_mean_df = sentiment_aggregator(df_aapl_vader_filtered, title = True, type = \"mean\") \n",
    "tsla_vader_mean_df = sentiment_aggregator(df_tsla_vader_filtered, title = True, type = \"mean\") \n",
    "\n",
    "df_world_vader_absmax = sentiment_aggregator(df_world_vader, title = True, type = \"abs_max\") \n",
    "df_politics_vader_absmax = sentiment_aggregator(df_politics_vader, title = True, type = \"abs_max\") \n",
    "df_coronavirus_vader_absmax = sentiment_aggregator(df_coronavirus_vader, title = True, type = \"abs_max\") \n",
    "\n",
    "df_world_vader_mean = sentiment_aggregator(df_world_vader, title = True, type = \"mean\") \n",
    "df_politics_vader_mean = sentiment_aggregator(df_politics_vader, title = True, type = \"mean\") \n",
    "df_coronavirus_vader_mean = sentiment_aggregator(df_coronavirus_vader, title = True, type = \"mean\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_finbert_maxabs_df.index = pd.to_datetime(meta_finbert_maxabs_df.index)\n",
    "aapl_finbert_maxabs_df.index = pd.to_datetime(aapl_finbert_maxabs_df.index)\n",
    "tsla_finbert_maxabs_df.index = pd.to_datetime(tsla_finbert_maxabs_df.index)\n",
    "\n",
    "meta_flair_df.index = pd.to_datetime(meta_flair_df.index)\n",
    "aapl_flair_df.index = pd.to_datetime(aapl_flair_df.index)\n",
    "tsla_flair_df.index = pd.to_datetime(tsla_flair_df.index)\n",
    "\n",
    "meta_vader_maxabs_df.index = pd.to_datetime(meta_vader_maxabs_df.index)\n",
    "aapl_vader_maxabs_df.index = pd.to_datetime(aapl_vader_maxabs_df.index)\n",
    "tsla_vader_maxabs_df.index = pd.to_datetime(tsla_vader_maxabs_df.index)\n",
    "\n",
    "meta_finbert_mean_df.index = pd.to_datetime(meta_finbert_mean_df.index)\n",
    "aapl_finbert_mean_df.index = pd.to_datetime(aapl_finbert_mean_df.index)\n",
    "tsla_finbert_mean_df.index = pd.to_datetime(tsla_finbert_mean_df.index)\n",
    "\n",
    "meta_vader_mean_df.index = pd.to_datetime(meta_vader_mean_df.index)\n",
    "aapl_vader_mean_df.index = pd.to_datetime(aapl_vader_mean_df.index)\n",
    "tsla_vader_mean_df.index = pd.to_datetime(tsla_vader_mean_df.index)\n",
    "\n",
    "df_world_finbert_maxabs.index=pd.to_datetime(df_world_finbert_maxabs.index)\n",
    "df_politics_finbert_maxabs.index=pd.to_datetime(df_politics_finbert_maxabs.index)\n",
    "df_coronavirus_finbert_maxabs.index=pd.to_datetime(df_coronavirus_finbert_maxabs.index)\n",
    "\n",
    "df_world_finbert_mean.index=pd.to_datetime(df_world_finbert_mean.index)\n",
    "df_politics_finbert_mean.index=pd.to_datetime(df_politics_finbert_mean.index)\n",
    "df_coronavirus_finbert_mean.index=pd.to_datetime(df_coronavirus_finbert_mean.index)\n",
    "\n",
    "df_world_flair.index=pd.to_datetime(df_world_flair.index)\n",
    "df_politics_flair.index=pd.to_datetime(df_politics_flair.index)\n",
    "df_coronavirus_flair.index=pd.to_datetime(df_coronavirus_flair.index)\n",
    "\n",
    "df_world_vader_absmax.index=pd.to_datetime(df_world_vader_absmax.index)\n",
    "df_politics_vader_absmax.index=pd.to_datetime(df_politics_vader_absmax.index)\n",
    "df_coronavirus_vader_absmax.index=pd.to_datetime(df_coronavirus_vader_absmax.index)\n",
    "\n",
    "df_world_vader_mean.index=pd.to_datetime(df_world_vader_mean.index)\n",
    "df_politics_vader_mean.index=pd.to_datetime(df_politics_vader_mean.index)\n",
    "df_coronavirus_vader_mean.index=pd.to_datetime(df_coronavirus_vader_mean.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_finbert_maxabs_merge=pd.merge(meta_hist,meta_finbert_maxabs_df, how='left', left_index=True, right_index=True)\n",
    "aapl_finbert_maxabs_merge=pd.merge(aapl_hist,aapl_finbert_maxabs_df, how='left', left_index=True, right_index=True)\n",
    "tsla_finbert_maxabs_merge=pd.merge(tsla_hist,tsla_finbert_maxabs_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "meta_vader_maxabs_merge=pd.merge(meta_hist,meta_vader_maxabs_df, how='left', left_index=True, right_index=True)\n",
    "aapl_vader_maxabs_merge=pd.merge(aapl_hist,aapl_vader_maxabs_df, how='left', left_index=True, right_index=True)\n",
    "tsla_vader_maxabs_merge=pd.merge(tsla_hist,tsla_vader_maxabs_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "meta_finbert_mean_merge=pd.merge(meta_hist,meta_finbert_mean_df, how='left', left_index=True, right_index=True)\n",
    "aapl_finbert_mean_merge=pd.merge(aapl_hist,aapl_finbert_mean_df, how='left', left_index=True, right_index=True)\n",
    "tsla_finbert_mean_merge=pd.merge(tsla_hist,tsla_finbert_mean_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "meta_flair_merge=pd.merge(meta_hist,meta_flair_df, how='left', left_index=True, right_index=True)\n",
    "aapl_flair_merge=pd.merge(aapl_hist,aapl_flair_df, how='left', left_index=True, right_index=True)\n",
    "tsla_flair_merge=pd.merge(tsla_hist,tsla_flair_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "meta_vader_mean_merge=pd.merge(meta_hist,meta_vader_mean_df, how='left', left_index=True, right_index=True)\n",
    "aapl_vader_mean_merge=pd.merge(aapl_hist,aapl_vader_mean_df, how='left', left_index=True, right_index=True)\n",
    "tsla_vader_mean_merge=pd.merge(tsla_hist,tsla_vader_mean_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# world\n",
    "\n",
    "df_world_finbert_maxabs_merge=pd.merge(meta_hist,df_world_finbert_maxabs, how='left', left_index=True, right_index=True)\n",
    "df_politics_finbert_maxabs_merge=pd.merge(meta_hist,df_politics_finbert_maxabs, how='left', left_index=True, right_index=True)\n",
    "df_coronavirus_finbert_maxabs_merge=pd.merge(meta_hist,df_coronavirus_finbert_maxabs, how='left', left_index=True, right_index=True)\n",
    "\n",
    "df_world_finbert_mean_merge=pd.merge(meta_hist,df_world_finbert_mean, how='left', left_index=True, right_index=True)\n",
    "df_politics_finbert_mean_merge=pd.merge(meta_hist,df_politics_finbert_mean, how='left', left_index=True, right_index=True)\n",
    "df_coronavirus_finbert_mean_merge=pd.merge(meta_hist,df_coronavirus_finbert_mean, how='left', left_index=True, right_index=True)\n",
    "\n",
    "df_world_flair_merge=pd.merge(meta_hist,df_world_flair, how='left', left_index=True, right_index=True)\n",
    "df_politics_flair_merge=pd.merge(meta_hist,df_politics_flair, how='left', left_index=True, right_index=True)\n",
    "df_coronavirus_flair_merge=pd.merge(meta_hist,df_coronavirus_flair, how='left', left_index=True, right_index=True)\n",
    "\n",
    "df_world_vader_maxabs_merge=pd.merge(meta_hist,df_world_vader_absmax, how='left', left_index=True, right_index=True)\n",
    "df_politics_vader_maxabs_merge=pd.merge(meta_hist,df_politics_vader_absmax, how='left', left_index=True, right_index=True)\n",
    "df_coronavirus_vader_maxabs_merge=pd.merge(meta_hist,df_coronavirus_vader_absmax, how='left', left_index=True, right_index=True)\n",
    "\n",
    "df_world_vader_mean_merge=pd.merge(meta_hist,df_world_vader_mean, how='left', left_index=True, right_index=True)\n",
    "df_politics_vader_mean_merge=pd.merge(meta_hist,df_politics_vader_mean, how='left', left_index=True, right_index=True)\n",
    "df_coronavirus_vader_mean_merge=pd.merge(meta_hist,df_coronavirus_vader_mean, how='left', left_index=True, right_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# f\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(meta_finbert_maxabs_merge)\n",
    "transformed = imputer.transform(meta_finbert_maxabs_merge)\n",
    "meta_finbert_maxabs_merge[:]= imputer.transform(meta_finbert_maxabs_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(aapl_finbert_maxabs_merge)\n",
    "transformed = imputer.transform(aapl_finbert_maxabs_merge)\n",
    "aapl_finbert_maxabs_merge[:]= imputer.transform(aapl_finbert_maxabs_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(tsla_finbert_maxabs_merge)\n",
    "transformed = imputer.transform(tsla_finbert_maxabs_merge)\n",
    "tsla_finbert_maxabs_merge[:]= imputer.transform(tsla_finbert_maxabs_merge)\n",
    "\n",
    "###\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(meta_flair_merge)\n",
    "transformed = imputer.transform(meta_flair_merge)\n",
    "meta_flair_merge[:]= imputer.transform(meta_flair_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(aapl_flair_merge)\n",
    "transformed = imputer.transform(aapl_flair_merge)\n",
    "aapl_flair_merge[:]= imputer.transform(aapl_flair_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(tsla_flair_merge)\n",
    "transformed = imputer.transform(tsla_flair_merge)\n",
    "tsla_flair_merge[:]= imputer.transform(tsla_flair_merge)\n",
    "\n",
    "###\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(meta_vader_maxabs_merge)\n",
    "transformed = imputer.transform(meta_vader_maxabs_merge)\n",
    "meta_vader_maxabs_merge[:]= imputer.transform(meta_vader_maxabs_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(aapl_vader_maxabs_merge)\n",
    "transformed = imputer.transform(aapl_vader_maxabs_merge)\n",
    "aapl_vader_maxabs_merge[:]= imputer.transform(aapl_vader_maxabs_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(tsla_vader_maxabs_merge)\n",
    "transformed = imputer.transform(tsla_vader_maxabs_merge)\n",
    "tsla_vader_maxabs_merge[:]= imputer.transform(tsla_vader_maxabs_merge)\n",
    "\n",
    "###\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(meta_finbert_mean_merge)\n",
    "transformed = imputer.transform(meta_finbert_mean_merge)\n",
    "meta_finbert_mean_merge[:]= imputer.transform(meta_finbert_mean_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(aapl_finbert_mean_merge)\n",
    "transformed = imputer.transform(aapl_finbert_mean_merge)\n",
    "aapl_finbert_mean_merge[:]= imputer.transform(aapl_finbert_mean_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(tsla_finbert_mean_merge)\n",
    "transformed = imputer.transform(tsla_finbert_mean_merge)\n",
    "tsla_finbert_mean_merge[:]= imputer.transform(tsla_finbert_mean_merge)\n",
    "\n",
    "###\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(meta_vader_mean_merge)\n",
    "transformed = imputer.transform(meta_vader_mean_merge)\n",
    "meta_vader_mean_merge[:]= imputer.transform(meta_vader_mean_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(aapl_vader_mean_merge)\n",
    "transformed = imputer.transform(aapl_vader_mean_merge)\n",
    "aapl_vader_mean_merge[:]= imputer.transform(aapl_vader_mean_merge)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputer.fit(tsla_vader_mean_merge)\n",
    "transformed = imputer.transform(tsla_vader_mean_merge)\n",
    "tsla_vader_mean_merge[:]= imputer.transform(tsla_vader_mean_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"AAPL\", \"META\", \"TSLA\", \"World\", \"Politics\", \"Coronavirus\"]\n",
    "\n",
    "lst = [meta_finbert_maxabs_merge['Sentiment'], aapl_finbert_maxabs_merge['Sentiment'], tsla_finbert_maxabs_merge['Sentiment'], df_world_finbert_maxabs_merge['Sentiment'], df_politics_finbert_maxabs_merge['Sentiment'], df_coronavirus_finbert_maxabs_merge['Sentiment']]\n",
    "df_all_maxabs_finbert = pd.concat(lst, keys=keys, axis=1)\n",
    "df_all_maxabs_finbert = df_all_maxabs_finbert.sort_values(by=\"Date\")\n",
    "\n",
    "lst = [meta_finbert_mean_merge['Compound'], aapl_finbert_mean_merge['Compound'], tsla_finbert_mean_merge['Compound'], df_world_finbert_mean_merge['Compound'], df_politics_finbert_mean_merge['Compound'], df_coronavirus_finbert_mean_merge['Compound']]\n",
    "df_all_mean_finbert = pd.concat(lst, keys=keys, axis=1)\n",
    "df_all_mean_finbert = df_all_mean_finbert.sort_values(by=\"Date\")\n",
    "\n",
    "lst = [meta_flair_merge['Score'], aapl_flair_merge['Score'], tsla_flair_merge['Score'], df_world_flair_merge['Score'], df_politics_flair_merge['Score'], df_coronavirus_flair_merge['Score']]\n",
    "df_all_flair = pd.concat(lst, keys=keys, axis=1)\n",
    "df_all_flair = df_all_flair.sort_values(by=\"Date\")\n",
    "\n",
    "lst = [meta_vader_maxabs_merge['Sentiment'], aapl_vader_maxabs_merge['Sentiment'], tsla_vader_maxabs_merge['Sentiment'], df_world_vader_maxabs_merge['Sentiment'], df_politics_vader_maxabs_merge['Sentiment'], df_coronavirus_vader_maxabs_merge['Sentiment']]\n",
    "df_all_maxabs_vader = pd.concat(lst, keys=keys, axis=1)\n",
    "df_all_maxabs_vader = df_all_maxabs_vader.sort_values(by=\"Date\")\n",
    "\n",
    "lst = [meta_vader_mean_merge['Compound'], aapl_vader_mean_merge['Compound'], tsla_vader_mean_merge['Compound'], df_world_vader_mean_merge['Compound'], df_politics_vader_mean_merge['Compound'], df_coronavirus_vader_mean_merge['Compound']]\n",
    "df_all_mean_vader = pd.concat(lst, keys=keys, axis=1)\n",
    "df_all_mean_vader = df_all_mean_vader.sort_values(by=\"Date\")\n",
    "\n",
    "df_all_maxabs_finbert.to_csv('../Data/2_Processed/Unstructured_Data/All/Filtered/all_finbert_maxabs.csv')\n",
    "df_all_mean_finbert.to_csv('../Data/2_Processed/Unstructured_Data/All/Filtered/all_finbert_mean.csv')\n",
    "df_all_flair.to_csv('../Data/2_Processed/Unstructured_Data/All/Filtered/all_flair.csv')\n",
    "df_all_maxabs_vader.to_csv('../Data/2_Processed/Unstructured_Data/All/Filtered/all_vader_maxabs.csv')\n",
    "df_all_mean_vader.to_csv('../Data/2_Processed/Unstructured_Data/All/Filtered/all_vader_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "992e3c1ac2fab5c819af78d2f68745e04c538b3e95445f87071a441f9b70fbfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
