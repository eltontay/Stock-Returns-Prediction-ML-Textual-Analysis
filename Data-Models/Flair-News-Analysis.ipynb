{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flairNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached flair-0.11.3-py3-none-any.whl (401 kB)\n",
      "Collecting gdown==4.4.0\n",
      "  Using cached gdown-4.4.0-py3-none-any.whl\n",
      "Collecting sentencepiece==0.1.95\n",
      "  Using cached sentencepiece-0.1.95.tar.gz (508 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting deprecated>=1.2.4\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (3.5.2)\n",
      "Collecting mpld3==0.3\n",
      "  Using cached mpld3-0.3-py3-none-any.whl\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Using cached konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.64.0)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2.0.0)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting janome\n",
      "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.11.0)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "Collecting pptree\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.1.1)\n",
      "Collecting gensim>=3.4.0\n",
      "  Using cached gensim-4.2.0-cp310-cp310-win_amd64.whl (23.9 MB)\n",
      "Collecting wikipedia-api\n",
      "  Using cached Wikipedia_API-0.5.4-py3-none-any.whl\n",
      "Requirement already satisfied: regex in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2022.9.13)\n",
      "Collecting conllu>=4.0\n",
      "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting segtok>=1.5.7\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.23.1)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: lxml in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.9.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\requests-2.28.1-py3.10.egg (from gdown==4.4.0->flair) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (3.8.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: six in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\six-1.16.0-py3.10.egg (from gdown==4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.23.4)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.14.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.4.0->flair) (1.8.1)\n",
      "Collecting Cython==0.29.28\n",
      "  Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Collecting networkx>=2.2\n",
      "  Using cached networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "Collecting py4j\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Using cached importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Using cached overrides-3.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (4.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.26.0->flair) (0.4.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.0.0->flair) (0.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.0.0->flair) (6.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\certifi-2022.6.15-py3.10.egg (from requests[socks]->gdown==4.4.0->flair) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\charset_normalizer-2.1.0-py3.10.egg (from requests[socks]->gdown==4.4.0->flair) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\idna-3.3-py3.10.egg (from requests[socks]->gdown==4.4.0->flair) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\urllib3-1.26.10-py3.10.egg (from requests[socks]->gdown==4.4.0->flair) (1.26.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.2.post1)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: sentencepiece\n",
      "  Building wheel for sentencepiece (setup.py): started\n",
      "  Building wheel for sentencepiece (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for sentencepiece\n",
      "Failed to build sentencepiece\n",
      "Installing collected packages: sentencepiece, py4j, pptree, overrides, mpld3, janome, wrapt, tabulate, smart-open, segtok, PySocks, networkx, more-itertools, langdetect, importlib-metadata, future, ftfy, Cython, conllu, cloudpickle, wikipedia-api, konoha, hyperopt, gensim, deprecated, gdown, bpemb, flair\n",
      "  Running setup.py install for sentencepiece: started\n",
      "  Running setup.py install for sentencepiece: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [22 lines of output]\n",
      "      c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "        warnings.warn(\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-3.10\n",
      "      creating build\\lib.win-amd64-3.10\\sentencepiece\n",
      "      copying src\\sentencepiece/__init__.py -> build\\lib.win-amd64-3.10\\sentencepiece\n",
      "      copying src\\sentencepiece/sentencepiece_model_pb2.py -> build\\lib.win-amd64-3.10\\sentencepiece\n",
      "      copying src\\sentencepiece/sentencepiece_pb2.py -> build\\lib.win-amd64-3.10\\sentencepiece\n",
      "      running build_ext\n",
      "      building 'sentencepiece._sentencepiece' extension\n",
      "      creating build\\temp.win-amd64-3.10\n",
      "      creating build\\temp.win-amd64-3.10\\Release\n",
      "      creating build\\temp.win-amd64-3.10\\Release\\src\n",
      "      creating build\\temp.win-amd64-3.10\\Release\\src\\sentencepiece\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\bin\\HostX86\\x64\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ic:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\include -Ic:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\Include -IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\include /EHsc /Tpsrc/sentencepiece/sentencepiece_wrap.cxx /Fobuild\\temp.win-amd64-3.10\\Release\\src/sentencepiece/sentencepiece_wrap.obj /MT /I..\\build\\root\\include\n",
      "      cl : Command line warning D9025 : overriding '/MD' with '/MT'\n",
      "      sentencepiece_wrap.cxx\n",
      "      c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\include\\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.25.28610\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for sentencepiece\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for sentencepiece did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [22 lines of output]\n",
      "      c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "        warnings.warn(\n",
      "      running install\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-3.10\n",
      "      creating build\\lib.win-amd64-3.10\\sentencepiece\n",
      "      copying src\\sentencepiece/__init__.py -> build\\lib.win-amd64-3.10\\sentencepiece\n",
      "      copying src\\sentencepiece/sentencepiece_model_pb2.py -> build\\lib.win-amd64-3.10\\sentencepiece\n",
      "      copying src\\sentencepiece/sentencepiece_pb2.py -> build\\lib.win-amd64-3.10\\sentencepiece\n",
      "      running build_ext\n",
      "      building 'sentencepiece._sentencepiece' extension\n",
      "      creating build\\temp.win-amd64-3.10\n",
      "      creating build\\temp.win-amd64-3.10\\Release\n",
      "      creating build\\temp.win-amd64-3.10\\Release\\src\n",
      "      creating build\\temp.win-amd64-3.10\\Release\\src\\sentencepiece\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\bin\\HostX86\\x64\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ic:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\include -Ic:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\Include -IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\include /EHsc /Tpsrc/sentencepiece/sentencepiece_wrap.cxx /Fobuild\\temp.win-amd64-3.10\\Release\\src/sentencepiece/sentencepiece_wrap.obj /MT /I..\\build\\root\\include\n",
      "      cl : Command line warning D9025 : overriding '/MD' with '/MT'\n",
      "      sentencepiece_wrap.cxx\n",
      "      c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\include\\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.25.28610\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> sentencepiece\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting git+https://github.com/zalandoresearch/flair.git\n",
      "  Cloning https://github.com/zalandoresearch/flair.git to c:\\users\\65831\\appdata\\local\\temp\\pip-req-build-u06x3yws\n",
      "  Resolved https://github.com/zalandoresearch/flair.git to commit 56ac673379fc3965d694b37b5aa3d09d7a4e7939\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pytorch-revgrad\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (1.11.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (4.9.1)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (2.8.2)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (3.5.2)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (0.10.1)\n",
      "Collecting janome\n",
      "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (2.0.0)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (1.1.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.0.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (4.23.1)\n",
      "Collecting segtok>=1.5.7\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting gdown==4.4.0\n",
      "  Using cached gdown-4.4.0-py3-none-any.whl\n",
      "Collecting deprecated>=1.2.4\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting mpld3==0.3\n",
      "  Using cached mpld3-0.3-py3-none-any.whl\n",
      "Collecting conllu>=4.0\n",
      "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting pptree\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair==0.11.3) (2022.9.13)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Using cached konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Collecting wikipedia-api\n",
      "  Using cached Wikipedia_API-0.5.4-py3-none-any.whl\n",
      "Collecting gensim>=3.4.0\n",
      "  Using cached gensim-4.2.0-cp310-cp310-win_amd64.whl (23.9 MB)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\requests-2.28.1-py3.10.egg (from gdown==4.4.0->flair==0.11.3) (2.28.1)\n",
      "Requirement already satisfied: six in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\six-1.16.0-py3.10.egg (from gdown==4.4.0->flair==0.11.3) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair==0.11.3) (4.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair==0.11.3) (3.8.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bpemb>=0.3.2->flair==0.11.3) (1.23.4)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.14.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.4.0->flair==0.11.3) (1.8.1)\n",
      "Collecting Cython==0.29.28\n",
      "  Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.8.1->flair==0.11.3) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.8.1->flair==0.11.3) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.8.1->flair==0.11.3) (6.0)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting networkx>=2.2\n",
      "  Using cached networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "Collecting py4j\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Using cached overrides-3.1.0-py3-none-any.whl\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Using cached importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.11.3) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.11.3) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.11.3) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.11.3) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.11.3) (4.34.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.21.3->flair==0.11.3) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.21.3->flair==0.11.3) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.26.0->flair==0.11.3) (0.4.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]>=4.0.0->flair==0.11.3) (0.13.1)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]>=4.0.0->flair==0.11.3) (3.19.4)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ftfy->flair==0.11.3) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.11.3) (3.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\certifi-2022.6.15-py3.10.egg (from requests[socks]->gdown==4.4.0->flair==0.11.3) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\charset_normalizer-2.1.0-py3.10.egg (from requests[socks]->gdown==4.4.0->flair==0.11.3) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\idna-3.3-py3.10.egg (from requests[socks]->gdown==4.4.0->flair==0.11.3) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\urllib3-1.26.10-py3.10.egg (from requests[socks]->gdown==4.4.0->flair==0.11.3) (1.26.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\65831\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair==0.11.3) (2.3.2.post1)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: flair\n",
      "  Building wheel for flair (pyproject.toml): started\n",
      "  Building wheel for flair (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for flair: filename=flair-0.11.3-py3-none-any.whl size=328141 sha256=ee997d140a0cdce9dcdf5dd555d276187b7bba341674d9f2a40aede5743d3456\n",
      "  Stored in directory: C:\\Users\\65831\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-r4ah6zzt\\wheels\\c3\\b6\\d9\\b89b752e6cfbbda77669b4568e6b1378a3f4422a1a6f1ec8bf\n",
      "Successfully built flair\n",
      "Installing collected packages: sentencepiece, py4j, pptree, overrides, mpld3, janome, wrapt, tabulate, smart-open, segtok, PySocks, networkx, more-itertools, langdetect, importlib-metadata, future, ftfy, Cython, conllu, cloudpickle, wikipedia-api, pytorch-revgrad, konoha, hyperopt, gensim, deprecated, gdown, bpemb, flair\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 5.0.0\n",
      "    Uninstalling importlib-metadata-5.0.0:\n",
      "      Successfully uninstalled importlib-metadata-5.0.0\n",
      "Successfully installed Cython-0.29.28 PySocks-1.7.1 bpemb-0.3.4 cloudpickle-2.2.0 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 future-0.18.2 gdown-4.4.0 gensim-4.2.0 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-9.0.0 mpld3-0.3 networkx-2.8.8 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 pytorch-revgrad-0.2.0 segtok-1.5.11 sentencepiece-0.1.97 smart-open-6.2.0 tabulate-0.9.0 wikipedia-api-0.5.4 wrapt-1.14.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/zalandoresearch/flair.git 'C:\\Users\\65831\\AppData\\Local\\Temp\\pip-req-build-u06x3yws'\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Alternative if above doesnt work\n",
    "# pip install --upgrade git+https://github.com/zalandoresearch/flair.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\65831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\65831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\65831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\65831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "import string\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    text = text.replace(\"'s\",\"\")\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def remove_special_character(text):\n",
    "    text = text.replace('\\n', ' ') \n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = ' '.join([lem.lemmatize(x, pos = 'v') for x in text.split()])\n",
    "    return corpus\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_special_character(text)\n",
    "    text = lemmatize(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:12:08,345 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to C:\\Users\\65831\\AppData\\Local\\Temp\\tmpskny8z6z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253M/253M [01:10<00:00, 3.75MB/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:13:20,088 copying C:\\Users\\65831\\AppData\\Local\\Temp\\tmpskny8z6z to cache at C:\\Users\\65831\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:13:20,905 removing temp file C:\\Users\\65831\\AppData\\Local\\Temp\\tmpskny8z6z\n",
      "2022-11-06 15:13:21,206 loading file C:\\Users\\65831\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 5.62kB/s]\n",
      "Downloading: 100%|██████████| 483/483 [00:00<00:00, 85.0kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:01<00:00, 226kB/s]  \n",
      "Downloading: 100%|██████████| 466k/466k [00:01<00:00, 323kB/s]  \n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_process(df) : \n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Processed Title'] = df['Title'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    df['Sentence Title'] = df['Processed Title'].apply(Sentence)\n",
    "    # df['Sentence Text'] = df['Processed Text'].apply(Sentence)\n",
    "    df['Sentence Title'].apply(classifier.predict)\n",
    "    # df['Sentence Text'].apply(classifier.predict)\n",
    "    \n",
    "    def function(x):\n",
    "        print(x)\n",
    "        return x.labels[0].value\n",
    "        \n",
    "    df['Label Title'] = df['Sentence Title'].apply(lambda x: function(x))\n",
    "    # df['Score'] = [-1 if x == 'NEGATIVE' else 1 for x in df['Label Title']]\n",
    "\n",
    "\n",
    "    # df['Label Text'] = df['Sentence Text'].apply(lambda x: x.labels[0].value)\n",
    "    # df['Score Text'] = df['Sentence Text'].apply(lambda x: x.labels[0].score)\n",
    "    # df.loc[df['Label Text'] == 'NEGATIVE', 'Score Text'] = 0 - df['Score Text']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_data():\n",
    "    df_world= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/world_news.csv')\n",
    "    df_politics= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/politics_news.csv')\n",
    "    df_coronavirus= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Global/coronavirus_news.csv')\n",
    "    df_aapl= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/aapl_news.csv')\n",
    "    df_meta= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/meta_news.csv')\n",
    "    df_tsla= pd.read_csv('../Data/1_Scraped/Unstructured_Data/Stock/tsla_news.csv')\n",
    "\n",
    "    # Removing duplicates\n",
    "    df_world = df_world.drop_duplicates(subset=['Title'], keep='first')\n",
    "    df_politics = df_politics.drop_duplicates(subset=['Title'], keep='first')\n",
    "    df_coronavirus = df_coronavirus.drop_duplicates(subset=['Title'], keep='first')\n",
    "    df_aapl = df_aapl.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    df_meta = df_meta.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    df_tsla = df_tsla.drop_duplicates(subset=['Date', 'Title'], keep='first')\n",
    "    \n",
    "    # Further filtering based on keyword\n",
    "    # aapl_keyword = ['Apple', 'iPhone', 'Macbook', 'iPad']\n",
    "    # meta_keyword = ['Facebook', 'Meta', 'Metaverse']\n",
    "    # tsla_keyword = ['Tesla', 'EV', 'Elon', 'Musk']\n",
    "\n",
    "    # df_aapl = df_aapl[df_aapl['Title'].str.contains('|'.join(aapl_keyword)) == True]\n",
    "    # df_meta = df_meta[df_meta['Title'].str.contains('|'.join(meta_keyword)) == True]\n",
    "    # df_tsla = df_tsla[df_tsla['Title'].str.contains('|'.join(tsla_keyword)) == True]\n",
    "\n",
    "    df_world = df_world.dropna()\n",
    "    df_politics = df_politics.dropna()\n",
    "    df_coronavirus = df_coronavirus.dropna()\n",
    "    df_aapl = df_aapl.dropna()\n",
    "    df_meta = df_meta.dropna()\n",
    "    df_tsla = df_tsla.dropna()\n",
    "    \n",
    "    return df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla\n",
    "\n",
    "def flair_all():\n",
    "    df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla = clean_all_data()\n",
    "    \n",
    "    # df_aapl = flair_process(df_aapl)\n",
    "    # print(\"aapl processed, starting meta\")\n",
    "    # df_meta = flair_process(df_meta)\n",
    "    # print(\"meta processed, starting tsla\")\n",
    "    # df_tsla = flair_process(df_tsla)\n",
    "    # print(\"tsla processed, starting world\")\n",
    "    df_world = flair_process(df_world)\n",
    "    # print(\"world processed, starting politics\")\n",
    "    # df_politics = flair_process(df_politics)\n",
    "    # print(\"politics processed, starting corona\")\n",
    "    # df_coronavirus = flair_process(df_coronavirus)\n",
    "    # print(\"corona processed\")\n",
    "\n",
    "    # df_aapl.to_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_flair.csv',index=False)\n",
    "    # df_meta.to_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_flair.csv',index=False)\n",
    "    # df_tsla.to_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_flair.csv',index=False)\n",
    "    df_world.to_csv('../Data/2_Processed/Unstructured_Data/Global/world_flair.csv',index=False)\n",
    "    # df_politics.to_csv('../Data/2_Processed/Unstructured_Data/Global/politics_flair.csv',index=False)\n",
    "    # df_coronavirus.to_csv('../Data/2_Processed/Unstructured_Data/GLobal/coronavirus_flair.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-06 18:48:30,916 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\Flair-News-Analysis.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla \u001b[39m=\u001b[39m clean_all_data()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_world \u001b[39m=\u001b[39m flair_process(df_world)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_world\n",
      "\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\Flair-News-Analysis.ipynb Cell 8\u001b[0m in \u001b[0;36mflair_process\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mSentence Title\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mProcessed Title\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(Sentence)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# df['Sentence Text'] = df['Processed Text'].apply(Sentence)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mSentence Title\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(classifier\u001b[39m.\u001b[39;49mpredict)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# df['Sentence Text'].apply(classifier.predict)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mLabel Title\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mSentence Title\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mlabels[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4248\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4252\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m   4254\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4255\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4355\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4357\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1093\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1099\u001b[0m             values,\n\u001b[0;32m   1100\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1101\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1102\u001b[0m         )\n\u001b[0;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1105\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:795\u001b[0m, in \u001b[0;36mDefaultClassifier.predict\u001b[1;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data_points:\n\u001b[0;32m    793\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_tensors(batch)\n\u001b[0;32m    797\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39mtensors)\n\u001b[0;32m    799\u001b[0m \u001b[39m# if anything could possibly be predicted\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:602\u001b[0m, in \u001b[0;36mDefaultClassifier._prepare_tensors\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[39mreturn\u001b[39;00m (torch\u001b[39m.\u001b[39mzeros(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_embedding_size, device\u001b[39m=\u001b[39mflair\u001b[39m.\u001b[39mdevice),)\n\u001b[0;32m    601\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_embeddings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inner_embeddings\u001b[39m.\u001b[39;49membed(filtered_data_points)\n\u001b[0;32m    603\u001b[0m embedding_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    604\u001b[0m \u001b[39mfor\u001b[39;00m prediction_data_point \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_prediction_data_points(filtered_data_points):\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\base.py:47\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m     44\u001b[0m     data_points \u001b[39m=\u001b[39m [data_points]\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_everything_embedded(data_points) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_embeddings:\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_embeddings_internal(data_points)\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m data_points\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:528\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings._add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_add_embeddings_internal\u001b[39m(\u001b[39mself\u001b[39m, sentences: List[Sentence]):\n\u001b[1;32m--> 528\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_tensors(sentences, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforce_device)\n\u001b[0;32m    529\u001b[0m     gradient_context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39menable_grad() \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_tune \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m    530\u001b[0m     \u001b[39mwith\u001b[39;00m gradient_context:\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:386\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings.prepare_tensors\u001b[1;34m(self, sentences, device)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39m# encode inputs\u001b[39;00m\n\u001b[0;32m    376\u001b[0m batch_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[0;32m    377\u001b[0m     flair_tokens,\n\u001b[0;32m    378\u001b[0m     stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m     is_split_into_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    384\u001b[0m )\n\u001b[1;32m--> 386\u001b[0m tensor_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__build_transformer_model_inputs(\n\u001b[0;32m    387\u001b[0m     batch_encoding, sentences, offsets, lengths, flair_tokens, device\n\u001b[0;32m    388\u001b[0m )\n\u001b[0;32m    390\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_args\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:421\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings.__build_transformer_model_inputs\u001b[1;34m(self, batch_encoding, sentences, offsets, sentence_lengths, flair_tokens, device)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    420\u001b[0m     cpu_overflow_to_sample_mapping \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m     lengths \u001b[39m=\u001b[39m (input_ids \u001b[39m!=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id)\u001b[39m.\u001b[39;49msum(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    422\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mlengths\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lengths\n\u001b[0;32m    424\u001b[0m \u001b[39m# set language IDs for XLM-style transformers\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla = clean_all_data()\n",
    "df_world = flair_process(df_world)\n",
    "df_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14/9/2022</td>\n",
       "      <td>U.S. Supreme Court risks its legitimacy by loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14/9/2022</td>\n",
       "      <td>Moderates fleeing U.S. House, setting stage fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14/9/2022</td>\n",
       "      <td>New Hampshire Republicans pick far-right candi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14/9/2022</td>\n",
       "      <td>U.S. senators introduce bill to designate Russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14/9/2022</td>\n",
       "      <td>U.S. distances itself from Bill Richardson mee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113817</th>\n",
       "      <td>8/2/2018</td>\n",
       "      <td>Clear Paris attacks suspect of Belgian shootin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113818</th>\n",
       "      <td>8/2/2018</td>\n",
       "      <td>U.S. envoy tells Lebanon Israel does not want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113819</th>\n",
       "      <td>8/2/2018</td>\n",
       "      <td>Ethiopia pardons 746 prisoners, including jour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113820</th>\n",
       "      <td>8/2/2018</td>\n",
       "      <td>Four-way meeting on Ukraine crisis in Munich n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113821</th>\n",
       "      <td>8/2/2018</td>\n",
       "      <td>Venezuela opposition ponders whether to fight ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70464 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                              Title\n",
       "0       14/9/2022  U.S. Supreme Court risks its legitimacy by loo...\n",
       "1       14/9/2022  Moderates fleeing U.S. House, setting stage fo...\n",
       "2       14/9/2022  New Hampshire Republicans pick far-right candi...\n",
       "3       14/9/2022  U.S. senators introduce bill to designate Russ...\n",
       "4       14/9/2022  U.S. distances itself from Bill Richardson mee...\n",
       "...           ...                                                ...\n",
       "113817   8/2/2018  Clear Paris attacks suspect of Belgian shootin...\n",
       "113818   8/2/2018  U.S. envoy tells Lebanon Israel does not want ...\n",
       "113819   8/2/2018  Ethiopia pardons 746 prisoners, including jour...\n",
       "113820   8/2/2018  Four-way meeting on Ukraine crisis in Munich n...\n",
       "113821   8/2/2018  Venezuela opposition ponders whether to fight ...\n",
       "\n",
       "[70464 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_world.drop_duplicates(subset=['Title'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-06 18:50:38,983 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\Flair-News-Analysis.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m flair_all()\n",
      "\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\Flair-News-Analysis.ipynb Cell 10\u001b[0m in \u001b[0;36mflair_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m df_world, df_politics, df_coronavirus, df_aapl, df_meta, df_tsla \u001b[39m=\u001b[39m clean_all_data()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# df_aapl = flair_process(df_aapl)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# print(\"aapl processed, starting meta\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# df_meta = flair_process(df_meta)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# print(\"meta processed, starting tsla\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# df_tsla = flair_process(df_tsla)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# print(\"tsla processed, starting world\")\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m df_world \u001b[39m=\u001b[39m flair_process(df_world)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# print(\"world processed, starting politics\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# df_politics = flair_process(df_politics)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# print(\"politics processed, starting corona\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# df_meta.to_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_flair.csv',index=False)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# df_tsla.to_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_flair.csv',index=False)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m df_world\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39m../Data/2_Processed/Unstructured_Data/Global/world_flair.csv\u001b[39m\u001b[39m'\u001b[39m,index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\65831\\Documents\\GitHub\\Stock-Returns-Prediction-ML-Textual-Analysis\\Data-Models\\Flair-News-Analysis.ipynb Cell 10\u001b[0m in \u001b[0;36mflair_process\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mSentence Title\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mProcessed Title\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(Sentence)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# df['Sentence Text'] = df['Processed Text'].apply(Sentence)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mSentence Title\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(classifier\u001b[39m.\u001b[39;49mpredict)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# df['Sentence Text'].apply(classifier.predict)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65831/Documents/GitHub/Stock-Returns-Prediction-ML-Textual-Analysis/Data-Models/Flair-News-Analysis.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunction\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4248\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4252\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m   4254\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4255\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4355\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4357\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1093\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1099\u001b[0m             values,\n\u001b[0;32m   1100\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1101\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1102\u001b[0m         )\n\u001b[0;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1105\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:795\u001b[0m, in \u001b[0;36mDefaultClassifier.predict\u001b[1;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data_points:\n\u001b[0;32m    793\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_tensors(batch)\n\u001b[0;32m    797\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39mtensors)\n\u001b[0;32m    799\u001b[0m \u001b[39m# if anything could possibly be predicted\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:602\u001b[0m, in \u001b[0;36mDefaultClassifier._prepare_tensors\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[39mreturn\u001b[39;00m (torch\u001b[39m.\u001b[39mzeros(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_embedding_size, device\u001b[39m=\u001b[39mflair\u001b[39m.\u001b[39mdevice),)\n\u001b[0;32m    601\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_embeddings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inner_embeddings\u001b[39m.\u001b[39;49membed(filtered_data_points)\n\u001b[0;32m    603\u001b[0m embedding_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    604\u001b[0m \u001b[39mfor\u001b[39;00m prediction_data_point \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_prediction_data_points(filtered_data_points):\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\base.py:47\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m     44\u001b[0m     data_points \u001b[39m=\u001b[39m [data_points]\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_everything_embedded(data_points) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_embeddings:\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_embeddings_internal(data_points)\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m data_points\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:531\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings._add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    529\u001b[0m gradient_context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39menable_grad() \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_tune \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m    530\u001b[0m \u001b[39mwith\u001b[39;00m gradient_context:\n\u001b[1;32m--> 531\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_tensors(tensors)\n\u001b[0;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_embedding:\n\u001b[0;32m    534\u001b[0m     document_embedding \u001b[39m=\u001b[39m embeddings[\u001b[39m\"\u001b[39m\u001b[39mdocument_embeddings\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:1128\u001b[0m, in \u001b[0;36mTransformerEmbeddings._forward_tensors\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_tensors\u001b[39m(\u001b[39mself\u001b[39m, tensors) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m-> 1128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtensors)\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:1036\u001b[0m, in \u001b[0;36mTransformerEmbeddings.forward\u001b[1;34m(self, input_ids, lengths, attention_mask, overflow_to_sample_mapping, word_ids, langs)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m attention_mask\n\u001b[1;32m-> 1036\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   1038\u001b[0m \u001b[39m# make the tuple a tensor; makes working with it easier.\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:567\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    568\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    569\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    570\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    574\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:345\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    343\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 345\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    346\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:300\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m    299\u001b[0m ffn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m ffn_output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer_norm(ffn_output \u001b[39m+\u001b[39;49m sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    302\u001b[0m output \u001b[39m=\u001b[39m (ffn_output,)\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\65831\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1104\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m-> 1104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1105\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[0;32m   1106\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "flair_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_aggregator(df, title = True):\n",
    "    # flair only gives one value and a label (POSITIVE or NEGATIVE) so we just use mean\n",
    "    # if title:\n",
    "    return df.groupby('Date')['Score'].aggregate('mean')\n",
    "\n",
    "    # else:\n",
    "    #     return df.groupby('Date')['Score Text'].aggregate('mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentiment_all(title):\n",
    "    df_world_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/world_flair.csv')\n",
    "    df_politics_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global/politics_flair.csv')\n",
    "    df_coronavirus_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Global//coronavirus_flair.csv')\n",
    "    df_aapl_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/aapl_flair.csv')\n",
    "    df_meta_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/meta_flair.csv')\n",
    "    df_tsla_flair= pd.read_csv('../Data/2_Processed/Unstructured_Data/Stock/tsla_flair.csv')\n",
    "\n",
    "    aggregated_sentiment_aapl = sentiment_aggregator(df_aapl_flair, title=title)\n",
    "    aggregated_sentiment_meta = sentiment_aggregator(df_meta_flair, title=title)\n",
    "    aggregated_sentiment_tsla = sentiment_aggregator(df_tsla_flair, title=title)\n",
    "    aggregated_sentiment_world = sentiment_aggregator(df_world_flair, title=title)\n",
    "    aggregated_sentiment_politics = sentiment_aggregator(df_politics_flair, title=title)\n",
    "    aggregated_sentiment_coronavirus = sentiment_aggregator(df_coronavirus_flair, title=title)\n",
    "\n",
    "    lst = [aggregated_sentiment_aapl, aggregated_sentiment_meta, aggregated_sentiment_tsla, aggregated_sentiment_world, aggregated_sentiment_politics, aggregated_sentiment_coronavirus]\n",
    "    keys = [\"AAPL\", \"META\", \"TSLA\", \"World\", \"Politics\", \"Coronavirus\"]\n",
    "    \n",
    "    return pd.concat(lst, keys=keys, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vader_all()\n",
    "df_all = aggregate_sentiment_all(title=True)\n",
    "df_all = df_all.sort_values(by=\"Date\")\n",
    "df_all.to_csv('../Data-Processed/all_flair.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "992e3c1ac2fab5c819af78d2f68745e04c538b3e95445f87071a441f9b70fbfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
