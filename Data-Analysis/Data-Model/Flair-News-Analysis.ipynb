{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in /usr/local/lib/python3.10/site-packages (0.11.3)\n",
      "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/site-packages (from flair) (0.5.4)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/site-packages (from flair) (4.9.1)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.10/site-packages (from flair) (4.2.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/site-packages (from flair) (9.0.0)\n",
      "Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.10/site-packages (from flair) (0.2.7)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.10/site-packages (from flair) (1.2.13)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/site-packages (from flair) (3.6.1)\n",
      "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.10/site-packages (from flair) (4.4.0)\n",
      "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.10/site-packages (from flair) (4.5.2)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/site-packages (from flair) (0.10.1)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/site-packages (from flair) (4.6.5)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.10/site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.10/site-packages (from flair) (4.23.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from flair) (2022.9.13)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/site-packages (from flair) (1.1.2)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/site-packages (from flair) (6.1.1)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.10/site-packages (from flair) (2.0.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/site-packages (from flair) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.10/site-packages (from flair) (4.64.1)\n",
      "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.10/site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/site-packages (from flair) (1.12.1)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.10/site-packages (from flair) (0.3.4)\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.10/site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.10/site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: pptree in /usr/local/lib/python3.10/site-packages (from flair) (3.1)\n",
      "Requirement already satisfied: janome in /usr/local/lib/python3.10/site-packages (from flair) (0.4.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown==4.4.0->flair) (3.8.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/site-packages (from gdown==4.4.0->flair) (2.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from bpemb>=0.3.2->flair) (1.23.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/site-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/site-packages (from gensim>=3.4.0->flair) (6.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/site-packages (from gensim>=3.4.0->flair) (1.9.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/site-packages (from hyperopt>=0.2.7->flair) (2.8.7)\n",
      "Requirement already satisfied: py4j in /usr/local/lib/python3.10/site-packages (from hyperopt>=0.2.7->flair) (0.10.9.7)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/site-packages (from hyperopt>=0.2.7->flair) (2.2.0)\n",
      "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /usr/local/lib/python3.10/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
      "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair) (4.37.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch!=1.8,>=1.5.0->flair) (4.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers>=4.0.0->flair) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers>=4.0.0->flair) (0.13.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==4.4.0->flair) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==4.4.0->flair) (2022.6.15.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==4.4.0->flair) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==4.4.0->flair) (3.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.2.post1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/eltontay/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eltontay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eltontay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/eltontay/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "import string\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world= pd.read_csv('../Data/News/Global/world_news.csv')\n",
    "df_politics= pd.read_csv('../Data/News/Global/politics_news.csv')\n",
    "df_coronavirus= pd.read_csv('../Data/News/Global/coronavirus_news.csv')\n",
    "df_aapl= pd.read_csv('../Data/News/Stock/aapl_news.csv')\n",
    "df_meta= pd.read_csv('../Data/News/Stock/meta_news.csv')\n",
    "df_tsla= pd.read_csv('../Data/News/Stock/tsla_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrelevant_content(text):\n",
    "    headline_only_string = \"This headline-only article is meant to show you why a stock is moving, the most difficult aspect of stock trading\"\n",
    "\n",
    "    if headline_only_string in text:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def remove_special_character(text):\n",
    "    text = text.replace('\\n', ' ') \n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in text]\n",
    "    return corpus\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_irrelevant_content(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = lowercase(text)\n",
    "    # text = remove_stopwords(text)\n",
    "    text = remove_special_character(text)\n",
    "    # text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-20 23:33:59,276 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to /var/folders/yv/9y1rb7mx4szd2ll5qlqxss040000gn/T/tmpdq45bnny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512723/265512723 [01:10<00:00, 3740441.30B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-20 23:35:11,192 copying /var/folders/yv/9y1rb7mx4szd2ll5qlqxss040000gn/T/tmpdq45bnny to cache at /Users/eltontay/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-20 23:35:11,271 removing temp file /var/folders/yv/9y1rb7mx4szd2ll5qlqxss040000gn/T/tmpdq45bnny\n",
      "2022-10-20 23:35:11,289 loading file /Users/eltontay/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 9.89kB/s]\n",
      "Downloading: 100%|██████████| 483/483 [00:00<00:00, 126kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:01<00:00, 231kB/s]  \n",
      "Downloading: 100%|██████████| 466k/466k [00:02<00:00, 192kB/s]  \n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_process(df) : \n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Processed Title'] = df['Title'].apply(lambda x: preprocess_text(x))\n",
    "    # df['Processed Text'] = df['Text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    df['Sentence Title'] = df['Processed Title'].apply(Sentence)\n",
    "    # df['Sentence Text'] = df['Processed Text'].apply(Sentence)\n",
    "    df['Sentence Title'].apply(classifier.predict)\n",
    "    # df['Sentence Text'].apply(classifier.predict)\n",
    "    \n",
    "    df['Label Title'] = df['Sentence Title'].apply(lambda x: x.labels[0].value)\n",
    "    df['Score Title'] = df['Sentence Title'].apply(lambda x: x.labels[0].score)\n",
    "    df.loc[df['Label Title'] == 'NEGATIVE', 'Score Title'] = 0 - df['Score Title']\n",
    "\n",
    "    # df['Label Text'] = df['Sentence Text'].apply(lambda x: x.labels[0].value)\n",
    "    # df['Score Text'] = df['Sentence Text'].apply(lambda x: x.labels[0].score)\n",
    "    # df.loc[df['Label Text'] == 'NEGATIVE', 'Score Text'] = 0 - df['Score Text']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_all():\n",
    "    df_world= pd.read_csv('../Data/News/Global/world_news.csv')\n",
    "    df_politics= pd.read_csv('../Data/News/Global/politics_news.csv')\n",
    "    df_coronavirus= pd.read_csv('../Data/News/Global/coronavirus_news.csv')\n",
    "    df_aapl= pd.read_csv('../Data/News/Stock/aapl_news.csv')\n",
    "    df_meta= pd.read_csv('../Data/News/Stock/meta_news.csv')\n",
    "    df_tsla= pd.read_csv('../Data/News/Stock/tsla_news.csv')\n",
    "    \n",
    "    df_aapl = flair_process(df_aapl)\n",
    "    print(\"aapl processed, starting meta\")\n",
    "    df_meta = flair_process(df_meta)\n",
    "    print(\"meta processed, starting tsla\")\n",
    "    df_tsla = flair_process(df_tsla)\n",
    "    print(\"tsla processed, starting world\")\n",
    "    df_world = flair_process(df_world)\n",
    "    print(\"world processed, starting politics\")\n",
    "    df_politics = flair_process(df_politics)\n",
    "    print(\"politics processed, starting corona\")\n",
    "    df_coronavirus = flair_process(df_coronavirus)\n",
    "    print(\"corona processed\")\n",
    "\n",
    "    df_aapl.to_csv('../Data-Processed/News/Stock/aapl_flair.csv',index=False)\n",
    "    df_meta.to_csv('../Data-Processed/News/Stock/meta_flair.csv',index=False)\n",
    "    df_tsla.to_csv('../Data-Processed/News/Stock/tsla_flair.csv',index=False)\n",
    "    df_world.to_csv('../Data-Processed/News/Global/world_flair.csv',index=False)\n",
    "    df_politics.to_csv('../Data-Processed/News/Global/politics_flair.csv',index=False)\n",
    "    df_coronavirus.to_csv('../Data-Processed/News/Global/coronavirus_flair.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl processed, starting meta\n",
      "meta processed, starting tsla\n",
      "tsla processed, starting world\n",
      "world processed, starting politics\n",
      "politics processed, starting corona\n",
      "corona processed\n"
     ]
    }
   ],
   "source": [
    "flair_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_aggregator(df, title = True):\n",
    "    # flair only gives one value and a label (POSITIVE or NEGATIVE) so we just use mean\n",
    "    # if title:\n",
    "    return df.groupby('Date')['Score Title'].aggregate('mean')\n",
    "\n",
    "    # else:\n",
    "    #     return df.groupby('Date')['Score Text'].aggregate('mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentiment_all(title):\n",
    "    df_world_flair= pd.read_csv('../Data-Processed/News/Global/world_flair.csv')\n",
    "    df_politics_flair= pd.read_csv('../Data-Processed/News/Global/politics_flair.csv')\n",
    "    df_coronavirus_flair= pd.read_csv('../Data-Processed/News/Global/coronavirus_flair.csv')\n",
    "    df_aapl_flair= pd.read_csv('../Data-Processed/News/Stock/aapl_flair.csv')\n",
    "    df_meta_flair= pd.read_csv('../Data-Processed/News/Stock/meta_flair.csv')\n",
    "    df_tsla_flair= pd.read_csv('../Data-Processed/News/Stock/tsla_flair.csv')\n",
    "\n",
    "    aggregated_sentiment_aapl = sentiment_aggregator(df_aapl_flair, title=title)\n",
    "    aggregated_sentiment_meta = sentiment_aggregator(df_meta_flair, title=title)\n",
    "    aggregated_sentiment_tsla = sentiment_aggregator(df_tsla_flair, title=title)\n",
    "    aggregated_sentiment_world = sentiment_aggregator(df_world_flair, title=title)\n",
    "    aggregated_sentiment_politics = sentiment_aggregator(df_politics_flair, title=title)\n",
    "    aggregated_sentiment_coronavirus = sentiment_aggregator(df_coronavirus_flair, title=title)\n",
    "\n",
    "    lst = [aggregated_sentiment_aapl, aggregated_sentiment_meta, aggregated_sentiment_tsla, aggregated_sentiment_world, aggregated_sentiment_politics, aggregated_sentiment_coronavirus]\n",
    "    keys = [\"AAPL\", \"META\", \"TSLA\", \"World\", \"Politics\", \"Coronavirus\"]\n",
    "    \n",
    "    return pd.concat(lst, keys=keys, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vader_all()\n",
    "df_all = aggregate_sentiment_all(title=True)\n",
    "df_all = df_all.sort_values(by=\"Date\")\n",
    "df_all.to_csv('../Data-Processed/all_flair.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
