{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
      "\u001b[K     |████████████████████████████████| 401 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pptree\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "Requirement already satisfied: regex in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from flair) (2022.7.9)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 91 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from flair) (0.2.1)\n",
      "Collecting gdown==4.4.0\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from flair) (3.5.1)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from flair) (4.64.0)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7 MB 499 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wikipedia-api\n",
      "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from flair) (1.0)\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "\u001b[K     |████████████████████████████████| 788 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from flair) (4.18.0)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 6.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting gensim>=3.4.0\n",
      "  Downloading gensim-4.2.0-cp37-cp37m-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 451 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting more-itertools\n",
      "  Downloading more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 2.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting lxml\n",
      "  Downloading lxml-4.9.1-cp37-cp37m-macosx_10_15_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 241 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting conllu>=4.0\n",
      "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from flair) (1.10.2)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from ftfy->flair) (0.2.5)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from hyperopt>=0.2.7->flair) (1.7.3)\n",
      "Requirement already satisfied: future in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Requirement already satisfied: six in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from hyperopt>=0.2.7->flair) (1.15.0)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from hyperopt>=0.2.7->flair) (1.21.5)\n",
      "Requirement already satisfied: filelock in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from huggingface-hub->flair) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from huggingface-hub->flair) (21.3)\n",
      "Requirement already satisfied: pyyaml in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from huggingface-hub->flair) (6.0)\n",
      "Requirement already satisfied: requests in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from huggingface-hub->flair) (2.28.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from huggingface-hub->flair) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from huggingface-hub->flair) (3.10.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (1.4.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from scikit-learn>=0.21.3->flair) (3.0.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers>=4.0.0->flair) (0.11.4)\n",
      "Requirement already satisfied: sacremoses in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers>=4.0.0->flair) (0.0.43)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 5.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests->huggingface-hub->flair) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests->huggingface-hub->flair) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests->huggingface-hub->flair) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests->huggingface-hub->flair) (2022.9.14)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.1)\n",
      "Requirement already satisfied: click in /Users/ashleyho/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from sacremoses->transformers>=4.0.0->flair) (8.0.4)\n",
      "Building wheels for collected packages: pptree, gdown, wikipedia-api, mpld3, sqlitedict, langdetect, overrides\n",
      "  Building wheel for pptree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=76bcd63117b2807fe9ffa6b68a13543e1c076c0633dace66fa641430c65ceed9\n",
      "  Stored in directory: /Users/ashleyho/Library/Caches/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14759 sha256=bcbf678ad17020a806a107409f20410e0a74d1bbd868183a015af12df031e08c\n",
      "  Stored in directory: /Users/ashleyho/Library/Caches/pip/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n",
      "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13462 sha256=7c8863c5ff544fb8f846b0ccad3636f998be37b629405e2fa4163bb7131436fa\n",
      "  Stored in directory: /Users/ashleyho/Library/Caches/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116678 sha256=165f34b48730b21f1a54639a669d1bbf7c243a44b275e792f73294f26c3d9d99\n",
      "  Stored in directory: /Users/ashleyho/Library/Caches/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15717 sha256=fae89f480fb03ee1c19965b1b0cccbfd679366025d6fa5ac53d2312c87d65994\n",
      "  Stored in directory: /Users/ashleyho/Library/Caches/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=bc1cade87ec01375931ccb68eca52f1315dfce45a81e82037eb12f48f8b1c2dc\n",
      "  Stored in directory: /Users/ashleyho/Library/Caches/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10173 sha256=dc7e6b671e7f4af889623e72835b3bb4c0247c4fb2643c52b3fe44dfee453442\n",
      "  Stored in directory: /Users/ashleyho/Library/Caches/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
      "Successfully built pptree gdown wikipedia-api mpld3 sqlitedict langdetect overrides\n",
      "\u001b[31mERROR: konoha 4.6.5 has requirement importlib-metadata<4.0.0,>=3.7.0, but you'll have importlib-metadata 4.8.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: sentencepiece, ftfy, pptree, cloudpickle, networkx, py4j, hyperopt, gdown, segtok, deprecated, tabulate, smart-open, gensim, bpemb, janome, wikipedia-api, overrides, konoha, mpld3, sqlitedict, more-itertools, lxml, conllu, langdetect, flair\n",
      "Successfully installed bpemb-0.3.4 cloudpickle-2.2.0 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 gdown-4.4.0 gensim-4.2.0 hyperopt-0.2.7 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 lxml-4.9.1 more-itertools-9.0.0 mpld3-0.3 networkx-2.6.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 segtok-1.5.11 sentencepiece-0.1.95 smart-open-6.2.0 sqlitedict-2.0.0 tabulate-0.9.0 wikipedia-api-0.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ashleyho/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashleyho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ashleyho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ashleyho/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "import string\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world= pd.read_csv('../Data/News/Global/world_news.csv')\n",
    "df_politics= pd.read_csv('../Data/News/Global/politics_news.csv')\n",
    "df_coronavirus= pd.read_csv('../Data/News/Global/coronavirus_news.csv')\n",
    "df_aapl= pd.read_csv('../Data/News/Stock/aapl_news.csv')\n",
    "df_meta= pd.read_csv('../Data/News/Stock/meta_news.csv')\n",
    "df_tsla= pd.read_csv('../Data/News/Stock/tsla_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrelevant_content(text):\n",
    "    headline_only_string = \"This headline-only article is meant to show you why a stock is moving, the most difficult aspect of stock trading\"\n",
    "\n",
    "    if headline_only_string in text:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def remove_special_character(text):\n",
    "    text = text.replace('\\n', ' ') \n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in text]\n",
    "    return corpus\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_irrelevant_content(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = lowercase(text)\n",
    "    # text = remove_stopwords(text)\n",
    "    text = remove_special_character(text)\n",
    "    # text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-20 12:26:00,742 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to /var/folders/zy/vb3z2d3x0jb2tc4swm_0dkph0000gn/T/tmpaxs46c2p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512723/265512723 [14:53<00:00, 297157.03B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-20 12:40:55,313 copying /var/folders/zy/vb3z2d3x0jb2tc4swm_0dkph0000gn/T/tmpaxs46c2p to cache at /Users/ashleyho/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-20 12:40:55,985 removing temp file /var/folders/zy/vb3z2d3x0jb2tc4swm_0dkph0000gn/T/tmpaxs46c2p\n",
      "2022-10-20 12:40:56,065 loading file /Users/ashleyho/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac8c60c0659424eb7a54e121cf88792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e7713984b648a88112ba4b38f39620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ead5ad418c745c69d4a3c4414ff1132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d21e209567e4e1e9ac48449a6005fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_process(df) : \n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Processed Title'] = df['Title'].apply(lambda x: preprocess_text(x))\n",
    "    df['Processed Text'] = df['Text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    df['Sentence Title'] = df['Processed Title'].apply(Sentence)\n",
    "    df['Sentence Text'] = df['Processed Text'].apply(Sentence)\n",
    "    df['Sentence Title'].apply(classifier.predict)\n",
    "    df['Sentence Text'].apply(classifier.predict)\n",
    "    \n",
    "    df['Label Title'] = df['Sentence Title'].apply(lambda x: x.labels[0].value)\n",
    "    df['Score Title'] = df['Sentence Title'].apply(lambda x: x.labels[0].score)\n",
    "    df.loc[df['Label Title'] == 'NEGATIVE', 'Score Title'] = 0 - df['Score Title']\n",
    "\n",
    "    df['Label Text'] = df['Sentence Text'].apply(lambda x: x.labels[0].value)\n",
    "    df['Score Text'] = df['Sentence Text'].apply(lambda x: x.labels[0].score)\n",
    "    df.loc[df['Label Text'] == 'NEGATIVE', 'Score Text'] = 0 - df['Score Text']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_all():\n",
    "    df_world= pd.read_csv('../Data/News/Global/world_news.csv')\n",
    "    df_politics= pd.read_csv('../Data/News/Global/politics_news.csv')\n",
    "    df_coronavirus= pd.read_csv('../Data/News/Global/coronavirus_news.csv')\n",
    "    df_aapl= pd.read_csv('../Data/News/Stock/aapl_news.csv')\n",
    "    df_meta= pd.read_csv('../Data/News/Stock/meta_news.csv')\n",
    "    df_tsla= pd.read_csv('../Data/News/Stock/tsla_news.csv')\n",
    "    \n",
    "    df_aapl = flair_process(df_aapl)\n",
    "    print(\"aapl processed, starting meta\")\n",
    "    df_meta = flair_process(df_meta)\n",
    "    print(\"meta processed, starting tsla\")\n",
    "    df_tsla = flair_process(df_tsla)\n",
    "    print(\"tsla processed, starting world\")\n",
    "    df_world = flair_process(df_world)\n",
    "    print(\"world processed, starting politics\")\n",
    "    df_politics = flair_process(df_politics)\n",
    "    print(\"politics processed, starting corona\")\n",
    "    df_coronavirus = flair_process(df_coronavirus)\n",
    "    print(\"corona processed\")\n",
    "\n",
    "    df_aapl.to_csv('../Data-Processed/News/Stock/aapl_flair.csv',index=False)\n",
    "    df_meta.to_csv('../Data-Processed/News/Stock/meta_flair.csv',index=False)\n",
    "    df_tsla.to_csv('../Data-Processed/News/Stock/tsla_flair.csv',index=False)\n",
    "    df_world.to_csv('../Data-Processed/News/Global/world_flair.csv',index=False)\n",
    "    df_politics.to_csv('../Data-Processed/News/Global/politics_flair.csv',index=False)\n",
    "    df_coronavirus.to_csv('../Data-Processed/News/Global/coronavirus_flair.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl processed, starting meta\n",
      "meta processed, starting tsla\n",
      "tsla processed, starting world\n",
      "world processed, starting politics\n",
      "politics processed, starting corona\n",
      "corona processed\n"
     ]
    }
   ],
   "source": [
    "flair_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_aggregator(df, title = True):\n",
    "    # flair only gives one value and a label (POSITIVE or NEGATIVE) so we just use mean\n",
    "    if title:\n",
    "        return df.groupby('Date')['Score Title'].aggregate('mean')\n",
    "\n",
    "    else:\n",
    "        return df.groupby('Date')['Score Text'].aggregate('mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentiment_all(title):\n",
    "    df_world_flair= pd.read_csv('../Data-Processed/News/Global/world_flair.csv')\n",
    "    df_politics_flair= pd.read_csv('../Data-Processed/News/Global/politics_flair.csv')\n",
    "    df_coronavirus_flair= pd.read_csv('../Data-Processed/News/Global/coronavirus_flair.csv')\n",
    "    df_aapl_flair= pd.read_csv('../Data-Processed/News/Stock/aapl_flair.csv')\n",
    "    df_meta_flair= pd.read_csv('../Data-Processed/News/Stock/meta_flair.csv')\n",
    "    df_tsla_flair= pd.read_csv('../Data-Processed/News/Stock/tsla_flair.csv')\n",
    "\n",
    "    aggregated_sentiment_aapl = sentiment_aggregator(df_aapl_flair, title=title)\n",
    "    aggregated_sentiment_meta = sentiment_aggregator(df_meta_flair, title=title)\n",
    "    aggregated_sentiment_tsla = sentiment_aggregator(df_tsla_flair, title=title)\n",
    "    aggregated_sentiment_world = sentiment_aggregator(df_world_flair, title=title)\n",
    "    aggregated_sentiment_politics = sentiment_aggregator(df_politics_flair, title=title)\n",
    "    aggregated_sentiment_coronavirus = sentiment_aggregator(df_coronavirus_flair, title=title)\n",
    "\n",
    "    lst = [aggregated_sentiment_aapl, aggregated_sentiment_meta, aggregated_sentiment_tsla, aggregated_sentiment_world, aggregated_sentiment_politics, aggregated_sentiment_coronavirus]\n",
    "    keys = [\"AAPL\", \"META\", \"TSLA\", \"World\", \"Politics\", \"Coronavirus\"]\n",
    "    \n",
    "    return pd.concat(lst, keys=keys, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>META</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>World</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Coronavirus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>-0.975312</td>\n",
       "      <td>-0.975312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.951166</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>-0.999898</td>\n",
       "      <td>-0.977946</td>\n",
       "      <td>-0.999833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>-0.999833</td>\n",
       "      <td>-0.999833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>-0.999833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.998165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>-0.999833</td>\n",
       "      <td>-0.974638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.536404</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.975060</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.996439</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.999210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.896383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1491 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      META      TSLA     World  Politics  Coronavirus\n",
       "Date                                                                     \n",
       "2018-01-31 -0.975312 -0.975312       NaN       NaN -0.951166          NaN\n",
       "2018-02-01 -0.999898 -0.977946 -0.999833       NaN       NaN          NaN\n",
       "2018-02-02 -0.999833 -0.999833       NaN       NaN       NaN          NaN\n",
       "2018-02-04 -0.999833       NaN       NaN -0.998165       NaN          NaN\n",
       "2018-02-05 -0.999833 -0.974638       NaN       NaN       NaN          NaN\n",
       "...              ...       ...       ...       ...       ...          ...\n",
       "2021-08-01       NaN       NaN       NaN       NaN -0.536404          NaN\n",
       "2021-09-05       NaN       NaN       NaN       NaN -0.975060          NaN\n",
       "2022-03-26       NaN       NaN       NaN       NaN -0.996439          NaN\n",
       "2020-04-11       NaN       NaN       NaN       NaN       NaN    -0.999210\n",
       "2020-05-23       NaN       NaN       NaN       NaN       NaN    -0.896383\n",
       "\n",
       "[1491 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_sentiment_all(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4878c0b1502401cbc64f38dc06f2fee10d3da6935f71a2e22c0988f3ac1b2885"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
